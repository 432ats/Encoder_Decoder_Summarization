{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moduel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (AdamW, \n",
    "                                               AlbertConfig, AlbertModel, AlbertTokenizer,\n",
    "                                               BertConfig, BertModel, BertTokenizer,\n",
    "                                               ElectraConfig, ElectraModel, ElectraTokenizer, ElectraForSequenceClassification,\n",
    "                                               RobertaConfig, RobertaModel, RobertaTokenizer, RobertaForSequenceClassification,\n",
    "                                               get_cosine_schedule_with_warmup,\n",
    "                                               get_linear_schedule_with_warmup)\n",
    "import nlp\n",
    "import logging\n",
    "from transformers import BertTokenizer, EncoderDecoderModel, Trainer, TrainingArguments\n",
    "import torch\n",
    "from transformers.tokenization_bert_japanese import BertJapaneseTokenizer\n",
    "from transformers.modeling_bert import BertForMaskedLM\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "from logging import INFO, FileHandler, Formatter, StreamHandler, getLogger\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('dataset_csv.csv', names = ('id', 'highlights', 'article'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>highlights</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>００００００４３</td>\n",
       "      <td>年末ジャンボ宝くじ第２９６回全国自治宝くじ当選番号</td>\n",
       "      <td>◇年末ジャンボ宝くじ第２９６回全国自治宝くじ（３１日・東京宝塚劇場）◇１等（６０００万円）３...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>００００００４４</td>\n",
       "      <td>ソウル市民歓喜、統一に希望の灯−−南北朝鮮・非核化共同宣言</td>\n",
       "      <td>【ソウル３１日薄木秀夫】南北朝鮮が三十一日、朝鮮半島の非核化の決意を世界に宣言した。ソウル市...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>００００００４５</td>\n",
       "      <td>出生率９．９に落ち込み人口１００人から子供１人生まれず−−人口動態年間推計</td>\n",
       "      <td>出生率（人口千人当たりの出生数）が一九九一年に遂に九・九まで落ち込み、十二年連続で史上最低記...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                             highlights  \\\n",
       "0  ００００００４３              年末ジャンボ宝くじ第２９６回全国自治宝くじ当選番号   \n",
       "1  ００００００４４          ソウル市民歓喜、統一に希望の灯−−南北朝鮮・非核化共同宣言   \n",
       "2  ００００００４５  出生率９．９に落ち込み人口１００人から子供１人生まれず−−人口動態年間推計   \n",
       "\n",
       "                                             article  \n",
       "0  ◇年末ジャンボ宝くじ第２９６回全国自治宝くじ（３１日・東京宝塚劇場）◇１等（６０００万円）３...  \n",
       "1  【ソウル３１日薄木秀夫】南北朝鮮が三十一日、朝鮮半島の非核化の決意を世界に宣言した。ソウル市...  \n",
       "2  出生率（人口千人当たりの出生数）が一九九一年に遂に九・九まで落ち込み、十二年連続で史上最低記...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    年末ジャンボ宝くじ第２９６回全国自治宝くじ当選番号\n",
       "1                ソウル市民歓喜、統一に希望の灯−−南北朝鮮・非核化共同宣言\n",
       "2        出生率９．９に落ち込み人口１００人から子供１人生まれず−−人口動態年間推計\n",
       "3                        内外とも多忙な年に天皇ご一家、健やかに新春\n",
       "4                  遭難の４人依然、不明−−伊豆諸島・八丈島沖のヨット事故\n",
       "                         ...                  \n",
       "86023                 ［雑記帳］ＪＲ大津駅構内の壁に「比良の天狗参上」\n",
       "86024                   田中貞美氏死去＝エスペランチスト護憲の会会長\n",
       "86025                       障害男性、年金減額恐れ自殺？−−大阪\n",
       "86026           大阪・福島区でアパート全焼、２人死亡独居高齢者入居、逃げ遅れ\n",
       "86027                          兵庫の農協でコメ４・２トン盗難\n",
       "Name: 1, Length: 86028, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[2][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # True にすると訓練データの最初の1000個だけ使う\n",
    "    'DEBUG': False,\n",
    "    # 乱数のシードを固定する値\n",
    "    'seed': 1234,\n",
    "    # pretrained list: https://huggingface.co/transformers/pretrained_models.html\n",
    "    'model_name': 'bert-base-uncased',\n",
    "    'model': EncoderDecoderModel,\n",
    "    'model_config': BertConfig,\n",
    "    'tokenizer': BertJapaneseTokenizer,\n",
    "    'do_lower_case': True,\n",
    "    # Bert 内部のパラメータに対する学習率\n",
    "    'encoder_lr': 1e-5,\n",
    "    # Bert 以外のパラメータに対する学習率\n",
    "    'decoder_lr': 1e-5,\n",
    "    # 学習エポック数\n",
    "    'epochs': 8,\n",
    "    # batch_size x accum_steps サンプルごとにパラメータ更新\n",
    "    'accum_steps': 1,\n",
    "    # ミニバッチサイズ\n",
    "    'batch_size': 16,\n",
    "    # 学習率を動的に変える仕組みの選択とそのパラメータ\n",
    "    'scheduler': 'linear', # ['linear', 'cosine', 'None']\n",
    "    'num_cycles': 0.5, # cosine(float)\n",
    "    'rate_warmup_steps': 0.15, # linear(int)\n",
    "    # 入力文の最大トークン数．これ以上は **切り捨てられる**\n",
    "    'encoder_max_length': 252,\n",
    "    'decoder_max_length': 64,\n",
    "    # 学習結果を保存するディレクトリ名\n",
    "    'output_dir': './src/output/',\n",
    "    # 学習結果を保存するファイル名\n",
    "    'save_model_name': 'bert_summarization.bin',\n",
    "    # ログファイルの prefix\n",
    "    'save_log_name': 'log',\n",
    "    # 入力データが存在する親ディレクトリ名\n",
    "    'input_dir': './input/',\n",
    "    # 入力ファイルが存在するディレクトリ名\n",
    "    # -> ファイルは input_dir + input_fname + {train_all.jsonl, dev.jsonl, test.jsonl}\n",
    "    # dataset from https://leaderboard.allenai.org/winogrande/submissions/get-started\n",
    "    'input_fname': 'winogrande_1.0/',\n",
    "    # mode = 'score' or 'loss' : 開発データ上で score (分類精度）かロスか，いずれが最良のものを保存するか指定\n",
    "    'mode': 'score',\n",
    "    # 搭載GPUを全部並列に使う（2020-09-08: matuzaki: hinoki で４枚GPU使うと二倍くらい速い．kayaX は未確認）\n",
    "    'parallel': True,\n",
    "    # Fine-tuning 済みのパラメータファイルを指定する\n",
    "    'fine_tuned_model': None # './src/output/bert_winogrande.bin'\n",
    "}\n",
    "\n",
    "config['verbose'] = 10 if config['DEBUG'] else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPUの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU が使えるときは使う\n",
    "config['device'] = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "config['num_device'] = torch.cuda.device_count()\n",
    "\n",
    "# 一度に各GPUで（元々の）batch_size ずつ並列処理\n",
    "if config['parallel']:\n",
    "    config['batch_size'] = config['batch_size'] * config['num_device']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乱数シードの固定\n",
    "def seed_everything(seed=42):\n",
    "    \"\"\"Function for consistency of experiment\n",
    "    input\n",
    "        seed: random seed\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ログ出力設定\n",
    "def get_logger(filename='log'):\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 変更点あり\n",
    "\n",
    "# Pandas のデータフレームとしてデータを読み込み．\n",
    "#\n",
    "# 訓練データは jsonl フォーマット（各行が一つの dictionary）で\n",
    "# ファイル名は以下のようにする\n",
    "#    訓練　 : 'input_dir' + 'input_fname' + train_all.jsonl\n",
    "#    開発　 : 'input_dir' + 'input_fname' + dev.jsonl\n",
    "#    テスト : 'input_dir' + 'input_fname' + test.jsonl\n",
    "def load_data(config):\n",
    "        \n",
    "    train = pd.read_json(config['input_dir']+config['input_fname']+'train_all.jsonl', orient='records', lines=True)\n",
    "    valid = pd.read_json(config['input_dir']+config['input_fname']+'dev.jsonl', orient='records', lines=True)\n",
    "    test =  pd.read_json(config['input_dir']+config['input_fname']+'test.jsonl', orient='records', lines=True)\n",
    "    \n",
    "    if config['DEBUG']:\n",
    "        train = train.head(1000)\n",
    "\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#変更点あり\n",
    "\n",
    "# array を2つ渡してその一致率として精度を計算する\n",
    "def get_score(label, prediction):\n",
    "    score = accuracy_score(label, prediction)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 変更点あり？\n",
    "\n",
    "# 最適化アルゴリズムのパラメータを設定\n",
    "def set_optimizer_params(model, config):\n",
    "    # Pre-trained モデル（Bert）のパラメータか自前の部分のものか判別するのに使用\n",
    "    def is_backbone(n):\n",
    "        return 'bert' in n\n",
    "\n",
    "    # モデルのパラメータを名前とともにリストにする\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "\n",
    "    # Weight decay を適用しないパラメータの名前のパターン\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    \n",
    "    # パラメータの種類ごとに最適化アルゴリズムのパラメータ（学習率・Weight decay の強さ）を設定\n",
    "    optimizer_parameters = [\n",
    "        # Bert の中のパラメータ．Weight decay させるもの\n",
    "        {\"params\": [p for n, p in param_optimizer if is_backbone(n) and not any(nd in n for nd in no_decay)],\n",
    "        \"lr\": config['encoder_lr'], \"weight_decay\": 0.01},\n",
    "        # Bert の中のパラメータ．Weight decay させないもの\n",
    "        {\"params\": [p for n, p in param_optimizer if is_backbone(n) and any(nd in n for nd in no_decay)],\n",
    "        \"lr\": config['encoder_lr'], \"weight_decay\": 0.0},\n",
    "        # Bert 以外のパラメータ．Weight decay させるもの\n",
    "        {\"params\": [p for n, p in param_optimizer if not is_backbone(n) and not any(nd in n for nd in no_decay)],\n",
    "        \"lr\": config['decoder_lr'], \"weight_decay\": 0.01},\n",
    "        # Bert 以外のパラメータ．Weight decay させないもの\n",
    "        {\"params\": [p for n, p in param_optimizer if not is_backbone(n) and any(nd in n for nd in no_decay)],\n",
    "        \"lr\": config['decoder_lr'], \"weight_decay\": 0.0,},\n",
    "    ]\n",
    "\n",
    "    return optimizer_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 変更点あり\n",
    "\n",
    "# Winogrande データの加工\n",
    "# \n",
    "# * sentence = \"John likes Mary because _ is nice.\"\n",
    "# * option1 = \"John\"\n",
    "# * option2 = \"Mary\"\n",
    "#\n",
    "# だったら，代名詞の位置（\"_\"）を先行詞候補で置き換えて\n",
    "#\n",
    "# * input1 -> \"[CLS] John likes Mary because John [SEP] is nice. [SEP]\"\n",
    "# * input2 -> \"[CLS] John likes Mary because Mary [SEP] is nice. [SEP]\"\n",
    "#\n",
    "# という２つの入力を作成する.\n",
    "#\n",
    "# また，config[\"max_length\"] を最大トークン数（特殊トークンも含む）で打ち切り，\n",
    "# それより短ければパディング（ゼロうめ）する\n",
    "#\n",
    "def convert_line(sentence, option1, option2, config, tokenizer):\n",
    "\n",
    "    # データバグを直す\n",
    "    if '_Laura' in sentence:\n",
    "        sentence = sentence.replace('_Laura', 'Laura')\n",
    "    elif '_ Matthew' in sentence:\n",
    "        sentence = sentence.replace('_ Matthew', 'Matthew')\n",
    "    \n",
    "    # アンダースコア \"_\" で表された代名詞の前後で文を分けて，先行詞候補ふたつ（option1, option2）をそれぞれ付加\n",
    "    sentence_s, sentence_e = sentence.split('_')\n",
    "    sentence_s_1 = ' '.join([sentence_s, option1])\n",
    "    sentence_s_2 = ' '.join([sentence_s, option2])\n",
    "\n",
    "    # [...置き換えた代名詞位置] + [SEP] + [代名詞位置より後] と [SEP] を挟んで結合し tokenize\n",
    "    inputs1 = tokenizer.encode_plus(sentence_s_1, text_pair=sentence_e, return_tensors=None, add_special_tokens=True, \n",
    "                    max_length=config['max_length'], pad_to_max_length=True, truncation=True)\n",
    "\n",
    "    # tokenizer からの出力（トークン番号のリスト，attension mask など）をすべて torch の tensor に変換して\n",
    "    # GPU 使用ならば GPU に置く\n",
    "    for k, v in inputs1.items():\n",
    "        inputs1[k] = torch.tensor(v, dtype=torch.long).to(config['device'])\n",
    "\n",
    "    # もう一つの選択肢をはめこんだほうも同様に tokenize\n",
    "    inputs2 = tokenizer.encode_plus(sentence_s_2, text_pair=sentence_e, return_tensors=None, add_special_tokens=True, \n",
    "                    max_length=config['max_length'], pad_to_max_length=True, truncation=True)\n",
    "    for k, v in inputs2.items():\n",
    "        inputs2[k] = torch.tensor(v, dtype=torch.long).to(config['device'])\n",
    "    \n",
    "    return inputs1, inputs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データを保持するクラス．DataLoader のためのインタフェースを提供\n",
    "class MyDataset:\n",
    "    # sentence, ..., answer: データの各フィールドおよび正解ラベルを縦に並べたリストをフィールドごとに与える\n",
    "    # config: 設定の dictionary\n",
    "    # tokenizer: 使用する transformer モデル（Bert ほか）用の tokenizer\n",
    "    def __init__(self, sentence, option1, option2, answer, config, tokenizer):\n",
    "        self.sentence = sentence\n",
    "        self.option1 = option1\n",
    "        self.option2 = option2\n",
    "        self.label = answer\n",
    "        self.config = config\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    # 全データ数を返す\n",
    "    def __len__(self):\n",
    "        return len(self.sentence)\n",
    "    \n",
    "    # item 番目のデータを返す\n",
    "    def __getitem__(self, item):\n",
    "        # トークナイズおよび必要な入力の加工\n",
    "        inputs1, inputs2 = convert_line(\n",
    "            self.sentence[item], \n",
    "            self.option1[item],\n",
    "            self.option2[item],\n",
    "            self.config,\n",
    "            self.tokenizer\n",
    "        )\n",
    "    \n",
    "        # 辞書の形でデータを返す．正解ラベルはゼロ始まりになるように -1 している\n",
    "        return {\n",
    "            'inputs1': inputs1,\n",
    "            'inputs2': inputs2,\n",
    "            'label': torch.tensor(self.label[item] - 1, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 epoch の学習\n",
    "#\n",
    "def trainer(model, data_loader, optimizer, criterion, scheduler, config):\n",
    "    # 勾配をためるモードに切り替え\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    preds = []\n",
    "    targets = []\n",
    "\n",
    "    if config['verbose']:\n",
    "        lossf = None\n",
    "        accf = None\n",
    "\n",
    "    # 勾配の和をクリア\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for idx, batch in enumerate(data_loader):\n",
    "        # モデルからの出力を logit として取得\n",
    "        pred = model(\n",
    "            batch['inputs1'],\n",
    "            batch['inputs2'],\n",
    "        )\n",
    "\n",
    "        # 正解と比較してロスを計算\n",
    "        loss =  criterion(pred, batch['label'].to(config['device']))\n",
    "\n",
    "        # バックプロパゲーション\n",
    "        loss.backward()\n",
    "\n",
    "        # 設定したミニバッチ数を処理したらパラメータ更新\n",
    "        if (idx + 1) % config['accum_steps'] == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # スケジューラ（学習率の動的調整）を進める\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # 経過報告のためにロスと訓練データに対する予測精度を保存\n",
    "        losses.append(loss.detach().cpu().item())\n",
    "        preds.extend(pred.cpu().detach().numpy().argmax(1))\n",
    "        targets.extend(batch[\"label\"].detach().cpu().numpy())\n",
    "        \n",
    "        # デバッグモードの時はミニバッチ10個ごとに経過を出力\n",
    "        if config['verbose']:\n",
    "            if lossf:\n",
    "                lossf = 0.98 * lossf + 0.02 * loss.item()\n",
    "            else:\n",
    "                lossf = loss.item()        \n",
    "            \n",
    "            if ((idx + 1) % config['verbose']) == 0:\n",
    "                logger.info(\"{} Train Loss : {:.4f}\".format(idx+1, lossf))\n",
    "    \n",
    "    return np.mean(losses), get_score(targets, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 開発データに対するロスと精度を計算\n",
    "#\n",
    "def evaluator(model, data_loader, criterion, config):\n",
    "    # 勾配をためないモード（評価用）に切り替え\n",
    "    model.eval()\n",
    "\n",
    "    losses = []\n",
    "    preds = []\n",
    "    targets = []\n",
    "\n",
    "    # 勾配計算はしない\n",
    "    with torch.no_grad():\n",
    "        # 開発データをミニバッチに分けてロスと分類精度を計算\n",
    "        for idx, batch in enumerate(data_loader):\n",
    "            pred = model(\n",
    "                batch['inputs1'],\n",
    "                batch['inputs2'],\n",
    "            )\n",
    "\n",
    "            loss = criterion(pred, batch['label'].to(config['device']))\n",
    "            \n",
    "            losses.append(loss.detach().cpu().item())\n",
    "            preds.extend(pred.cpu().detach().numpy().argmax(1))\n",
    "            targets.extend(batch[\"label\"].detach().cpu().numpy())\n",
    "    \n",
    "    return np.mean(losses), get_score(targets, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained モデルの上にかぶせる個別タスク用のモデルクラス\n",
    "#\n",
    "# 1. ２つの文（２つの選択肢に対応）を Bert にそれぞれ入力する\n",
    "#\n",
    "# 2. それぞれの [CLS] トークンに対応する出力ベクトルを\n",
    "#    (BertModel 内部で）全結合層＋tanh に通したものを得る\n",
    "#    * 以下のコメントではこれを単に「[CLS] トークンに対応するベクトル」と呼ぶ\n",
    "#    * ここで使われる全結合層は next sentence prediction で Pre-training されたもの\n",
    "#\n",
    "# 3. ２文の [CLS] トークンに対応するベクトルとパラメータベクトルの内積を取ったものを\n",
    "#    softmax に入れて各選択肢が正しい確率とみなす\n",
    "#\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, config, model_config):\n",
    "        super(CustomModel, self).__init__()\n",
    "\n",
    "        # Pre-trained モデルのパラメータを読み込み\n",
    "        self.bert = config['model'].from_pretrained(config['model_name'], config=model_config)\n",
    "\n",
    "        # [CLS] トークンに対する出力ベクトルに適用する Drop-out レイヤ\n",
    "        # drop-out 確率は Pre-trained モデルの訓練と同一のものを使用（？）\n",
    "        self.dropout = nn.Dropout(model_config.hidden_dropout_prob)\n",
    "\n",
    "        # [CLS] トークンに対する出力ベクトルと内積を取るパラメータベクトル\n",
    "        self.classifier = nn.Linear(model_config.hidden_size, 1)\n",
    "\n",
    "        # config で fine_tuned_model が\n",
    "        # 指定された   -> それを読み込む\n",
    "        # 指定されない -> Bert の pre-trained モデルを読み込み付加するパラメータを初期化\n",
    "        if config[\"fine_tuned_model\"]:\n",
    "            # 空の Bert モデルを作る\n",
    "            self.bert = config['model'](config=model_config)\n",
    "\n",
    "            # Fine-tuning 済みのパラメータを読み込む\n",
    "            self.load_state_dict(torch.load(config[\"fine_tuned_model\"]))\n",
    "        else:\n",
    "            # Pre-trained モデルのパラメータを読み込み\n",
    "            self.bert = config['model'].from_pretrained(config['model_name'], config=model_config)\n",
    "\n",
    "            # 分類のためのパラメータベクトルを正規分布でランダム初期化\n",
    "            nn.init.normal_(self.classifier.weight, std=0.02)\n",
    "    \n",
    "    # 確率値が欲しい時は forward の引数で softmax = True とする\n",
    "    # それ以外のときは softmax をとる前の値（選択肢ごとのスコア = logit）を返す\n",
    "    def forward(self, input1, input2, softmax=False):\n",
    "        # ２つの選択肢に対応した２つの入力をそれぞれ Bert に通して [CLS] トークンに\n",
    "        # 対応したベクトルを取り出す\n",
    "        _, h1 = self.bert(**input1)\n",
    "        _, h2 = self.bert(**input2)\n",
    "\n",
    "        # Drop-out を適用してからパラメータベクトルと内積をとる\n",
    "        h1 = self.dropout(h1)\n",
    "        y_pred1 = self.classifier(h1)\n",
    "        \n",
    "        # もう片方の文に対する出力についても同じ\n",
    "        h2 = self.dropout(h2)\n",
    "        y_pred2 = self.classifier(h2)\n",
    "        \n",
    "        # ミニバッチ内の各サンプルに対する y_pred1, y_pred2 を並べて\n",
    "        # batch_size x 2 の tensor にする\n",
    "        logits = torch.cat((y_pred1.reshape(-1, 1), y_pred2.reshape(-1, 1)), dim=1)\n",
    "\n",
    "        if softmax:\n",
    "            logits = F.softmax(logits, dim=1)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger(config['output_dir'] + config['save_log_name'])\n",
    "\n",
    "def main():\n",
    "\n",
    "    logger.info(\"Config\")\n",
    "    for k, v in config.items():\n",
    "        logger.info(f'   {k}:{v}')\n",
    "\n",
    "    # データをファイルから読み込み\n",
    "    train, valid, test = load_data(config)\n",
    "    logger.info(\"train: {}, valid:{}, test: {}\".format(len(train), len(valid), len(test)))\n",
    "\n",
    "    # トークナイザを読み込み\n",
    "    tokenizer = config['tokenizer'].from_pretrained(config['model_name'], do_lower_case=config['do_lower_case'])\n",
    "\n",
    "    # 実験結果が再現できるように乱数シードを固定\n",
    "    seed_everything(config['seed'])\n",
    "\n",
    "    # 訓練データの DataLoader を準備\n",
    "    train_dataset = MyDataset(train['sentence'].values, train['option1'].values, train['option2'].values, train['answer'].values, config, tokenizer)\n",
    "    # 単に２文から正しいものを選ぶ場合\n",
    "    # train_dataset = MyDataset(train['sentence1'].values, train['sentence2'].values, train['answer'].values, config, tokenizer)\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=config['batch_size'])\n",
    "\n",
    "    # 開発データの DataLoader を準備\n",
    "    valid_dataset = MyDataset(valid['sentence'].values, valid['option1'].values, valid['option2'].values, valid['answer'].values, config, tokenizer)\n",
    "    # 単に２文から正しいものを選ぶ場合\n",
    "    # valid_dataset = MyDataset(valid['sentence1'].values, valid['sentence2'].values, valid['answer'].values, config, tokenizer)\n",
    "    valid_loader = DataLoader(valid_dataset, shuffle=False, batch_size=config['batch_size'])\n",
    "\n",
    "    # Pre-trained モデルの設定を読み込み\n",
    "    model_config = config['model_config'].from_pretrained(config['model_name'])\n",
    "\n",
    "    # Pre-trained モデルを自分のモデルに読み込む\n",
    "    model = CustomModel(config, model_config)\n",
    "\n",
    "    # 複数 GPU を並列に使う場合\n",
    "    if config['parallel']:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # 勾配をクリアして GPU に送る\n",
    "    model.zero_grad()\n",
    "    model.to(config['device'])\n",
    "\n",
    "    # 最適化アルゴリズム（AdamW）の設定\n",
    "    optimizer_grouped_parameters = set_optimizer_params(model, config)\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=config['encoder_lr'], eps=1e-6)\n",
    "\n",
    "    # パラメータ更新回数\n",
    "    num_train_steps = math.floor(len(train_dataset)  * config['epochs'] / config['batch_size'])\n",
    "\n",
    "    # スケジューラーの設定（パラメータ更新につれて学習率を調節する仕組み）\n",
    "    if config['scheduler'] == 'linear':\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=math.floor(config['rate_warmup_steps'] * num_train_steps), num_training_steps=num_train_steps\n",
    "        )\n",
    "    elif config['scheduler'] == 'cosine':\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=math.floor(config['rate_warmup_steps'] * num_train_steps), num_training_steps=num_train_steps, num_cycles=config['num_cycles']\n",
    "        )\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    # ロスはモデルからの logit の出力を softmax に通した確率の負の対数尤度\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_score = 0.\n",
    "    best_loss = 100.\n",
    "\n",
    "    for epoch in range(config['epochs']):\n",
    "        start = time.time()\n",
    "\n",
    "        # 1 epoch 分パラメータ更新\n",
    "        train_loss, train_score = trainer(model, train_loader, optimizer, criterion, scheduler, config)\n",
    "\n",
    "        # 開発データ上でのロスと分類精度を測る\n",
    "        val_loss, val_score = evaluator(model, valid_loader, criterion, config)\n",
    "\n",
    "        logger.info(\n",
    "            \"Epoch {}: Train Loss {:.4f}, Valid Loss {:.4f}, Train score {:.4f}, Valid score {:.4f}, elapsed {:.4f}s\".format(\n",
    "                epoch + 1, train_loss, val_loss, train_score, val_score, time.time() - start)\n",
    "            )\n",
    "        \n",
    "        # 設定に従って，分類精度かロスかがこれまでで最良ならモデルを保存\n",
    "        new_best = False\n",
    "        if config['mode'] == 'score':\n",
    "            if val_score > best_score:\n",
    "                new_best = True\n",
    "        elif config['mode'] == 'loss':\n",
    "            if val_loss < best_loss:\n",
    "                new_best = True\n",
    "\n",
    "        if new_best:\n",
    "            best_loss = val_loss\n",
    "            best_score = val_score\n",
    "            if config['parallel']:\n",
    "                torch.save(model.module.state_dict(), config['output_dir'] + config['save_model_name'])\n",
    "            else:\n",
    "                torch.save(model.state_dict(), config['output_dir'] + config['save_model_name'])\n",
    "    \n",
    "    logger.info('End of experiment!')\n",
    "    logger.info('best loss: {:.4f}, score: {:.4f}'.format(best_loss, best_score))\n",
    "    logger.info('')\n",
    "\n",
    "    # GPU のメモリを解放\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TestDataの評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test():\n",
    "\n",
    "    logger.info(\"Config\")\n",
    "    for k, v in config.items():\n",
    "        logger.info(f'   {k}:{v}')\n",
    "\n",
    "    # データをファイルから読み込み\n",
    "    _, _, test = load_data(config)\n",
    "    logger.info(\"# test instances: {}\".format(len(test)))\n",
    "\n",
    "    # トークナイザを読み込み\n",
    "    tokenizer = config['tokenizer'].from_pretrained(config['model_name'], do_lower_case=config['do_lower_case'])\n",
    "\n",
    "    # 実験結果が再現できるように乱数シードを固定\n",
    "    seed_everything(config['seed'])\n",
    "\n",
    "    # テストデータの DataLoader を準備\n",
    "    test_dataset = MyDataset(test['sentence'].values, test['option1'].values, test['option2'].values, test['answer'].values, config, tokenizer)\n",
    "    test_loader = DataLoader(test_dataset, shuffle=False, batch_size=config['batch_size'])\n",
    "\n",
    "    # Pre-trained モデルの設定を読み込み\n",
    "    model_config = config['model_config'].from_pretrained(config['model_name'])\n",
    "\n",
    "    # Fine-tuning 済みのモデルを自分のモデルに読み込む\n",
    "    model = CustomModel(config, model_config)\n",
    "\n",
    "    # 複数 GPU を並列に使う場合\n",
    "    if config['parallel']:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # 勾配をクリアして GPU に送る\n",
    "    model.zero_grad()\n",
    "    model.to(config['device'])\n",
    "\n",
    "    # ロスはモデルからの logit の出力を softmax に通した確率の負の対数尤度\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # テストデータ上でのロスと分類精度を測る\n",
    "    test_loss, test_score = evaluator(model, test_loader, criterion, config)\n",
    "\n",
    "    logger.info(\n",
    "        \"Test Loss {:.4f}, Test score {:.4f}, elapsed {:.4f}s\".format(\n",
    "                test_loss, test_score, time.time() - start)\n",
    "            )\n",
    "        \n",
    "    logger.info('End of test!')\n",
    "    logger.info('')\n",
    "\n",
    "    # GPU のメモリを解放\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if config[\"fine_tuned_model\"]:\n",
    "        run_test()\n",
    "    else:\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
